{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction to Machine Learning Final Open Test**\n",
    "- Student ID: 19127191\n",
    "- Student name: Ngo Van Anh Kiet\n",
    "- Class: 19KHMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import classification_report\n",
    "from dataclasses import dataclass\n",
    "# To run gradient descent on the cost function optimally\n",
    "from scipy import optimize\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The concept of overfitting\n",
    "Overfitting is a phenomenal in machine learning where a learning model fits too much to the training set, which causes the model to lose its generalizability and flexibility. This can lead to incorrect predictions when the model meets some new data it has never seen before.\n",
    "There are many reasons for overfitting, for examples:\n",
    "- There's not enough training data;\n",
    "- The training the data isn't diverse;\n",
    "- The learning model is too complex;\n",
    "- Redundant features are used in the training process.\n",
    "\n",
    "Nowadays, many machine learning models have to worry about overfitting and implement many ways to handle this problem. This report introduces one of the popular methods to reduce overfitting that is the L2 Regularization technique. To be specific, The report will show how this regularization technique can be applied in the Logistic Regression algorithm used for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. A classification problem\n",
    "Suppose we have a dataset with 100 samples, each sample has 2 features: $X_1$ and $X_2$, along with a label $y$, which can have the values 0 or 1 (true or false). This is a binary classification problem often seen in many real-life situations. We can use a logistic regression model to solve this problem.\n",
    "\n",
    "First of all, let's create a random toy dataset to illustrate the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (100, 2)\n",
      "y: (100,)\n",
      "Sample X[0]: [-0.17246763 -0.55817206]\n",
      "Sample y[0]: 1\n"
     ]
    }
   ],
   "source": [
    "# Random dataset of 100 samples with binary labels\n",
    "X, y = make_circles(100, noise=0.15, factor=0.5, random_state=42)\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)\n",
    "print(\"Sample X[0]:\", X[0])\n",
    "print(\"Sample y[0]:\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the dataset on a scatter plot to see how the decision boundary (the line which separates the data points by their labels) might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X, y, ax):\n",
    "    \"\"\"Plot the data points X with labels y into a figure/subplot. The symbol '*' is for positive samples and 'o' is for the negative samples.\n",
    "\n",
    "    Args:\n",
    "        X (array_like): An M x 2 matrix representing the dataset features.\n",
    "        y (array_like): Label values for the dataset. A vector of size M x 1.\n",
    "        ax: The subplot to plot the data on\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find indices of positive and negative samples\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    \n",
    "    # Plot the samples\n",
    "    ax.plot(X[pos, 0], X[pos, 1], 'k*', lw=2, ms=10)\n",
    "    ax.plot(X[neg, 0], X[neg, 1], 'ko', mfc='y', ms=8, mec='k', mew=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data on a scatter plot using plotData function\n",
    "fig, ax = plt.subplots()\n",
    "plotData(X, y, ax)\n",
    "# Set labels and legend for the plot\n",
    "ax.set(xlabel=\"Feature X_1\", ylabel=\"Feature X_2\")\n",
    "ax.legend([\"y = 1\", \"y = 0\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the decision boundary must have a shape similar to a circle, so that the positive samples are inside the circle while the negative ones are outside.  \n",
    "Clearly, this decision boudary can't be fitted linearly with just 2 features. Usually, some feature engineering can be done to create new features for the dataset. Here we'll use the Feature Mapping technique to map the dataset from the original 2 features to quadratic features up to the 8th polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(X1, X2, degree=6):\n",
    "    \"\"\"Map the two input features to quadratic features.\n",
    "    \n",
    "    Return a new dataset with more features, comprising of:\n",
    "    X1, X2, X1^2, X2^2, X1*X2, X1*X2^2,...\n",
    "\n",
    "    Args:\n",
    "        X1 (array_like): A vector of shape (m, 1) containing the first feature.\n",
    "        \n",
    "        X2 (array_like): A vector of shape (m, 1) containing the second feature. X1 and X2 must be the same size.\n",
    "        \n",
    "        degree (int, optional): The polynomial degree. Defaults to 6.\n",
    "    \"\"\"\n",
    "    if X1.ndim > 0:\n",
    "        out = [np.ones(X1.shape[0])]\n",
    "    else:\n",
    "        out = [np.ones(1)]\n",
    "        \n",
    "    for i in range(1, degree + 1):\n",
    "        for j in range(i + 1):\n",
    "            out.append((X1 ** (i - j)) * (X2 ** j))\n",
    "            \n",
    "    if X1.ndim > 0:\n",
    "        return np.stack(out, axis=1)\n",
    "    else:\n",
    "        return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (100, 45)\n",
      "Sample X[0]: [ 1.00000000e+00 -1.72467631e-01 -5.58172063e-01  2.97450838e-02\n",
      "  9.62666135e-02  3.11556052e-01 -5.13006415e-03 -1.66028748e-02\n",
      " -5.37333343e-02 -1.73901884e-01  8.84770012e-04  2.86345849e-03\n",
      "  9.26726088e-03  2.99924460e-02  9.70671735e-02 -1.52594188e-04\n",
      " -4.93853903e-04 -1.59830253e-03 -5.17272612e-03 -1.67409455e-02\n",
      " -5.41801845e-02  2.63175582e-05  8.51738128e-05  2.75655452e-04\n",
      "  8.92127822e-04  2.88727121e-03  9.34432808e-03  3.02418654e-02\n",
      " -4.53892692e-06 -1.46897257e-05 -4.75416428e-05 -1.53863172e-04\n",
      " -4.97960827e-04 -1.61159413e-03 -5.21574288e-03 -1.68801644e-02\n",
      "  7.82817974e-07  2.53350220e-06  8.19939452e-06  2.65364168e-05\n",
      "  8.58821242e-05  2.77947822e-04  8.99546820e-04  2.91128196e-03\n",
      "  9.42203617e-03]\n"
     ]
    }
   ],
   "source": [
    "X = mapFeature(X[:, 0], X[:, 1], 8)\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Sample X[0]:\", X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset has been mapped from 2 features to 45 features. Surely with this new training data, our model can fit quite well to the complex shape of the decision boundary. However, as mentioned before, one of the reasons for overfitting is using redundant features in the training process. Since we now have so many features, overfitting is unavoidable. Thus, we can apply the L2 regularization technique to reduce overfitting in the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. L2 Regularization\n",
    "Let's review the sigmoid function, loss function and gradient function in logistic regression:\n",
    "- Sigmoid function: $$sigmoid(x)=\\frac{1}{1+e^{-x}}$$\n",
    "- Loss function:\n",
    "$$\n",
    "J(\\theta)= \\frac{1}{m} \\sum_{i=1}^{m} \\left[\n",
    "    -y^{(i)} log \\left(\n",
    "        h_\\theta(x^{(i)})\n",
    "    \\right)\n",
    "    - (1-y^{(i)}) log \\left(\n",
    "        1-h_\\theta(x^{(i)})\n",
    "    \\right)\n",
    "\\right]\n",
    "$$\n",
    "- Gradient function:\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m}\n",
    "\\left(\n",
    "    h_\\theta(x^{(i)}) - y^{(i)}\n",
    "\\right)\n",
    "x_{j}^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what is L2 regularization, and how is it applied to reduce overfitting?**  \n",
    "L2 regularization is a technique to reduce overfitting in machine learning fields like regression or neural networks. In regression, it's called *Ridge Regression*, while in neural networks, it's also known as *Weight Decay*.  \n",
    "This technique is actually just a new term added into the original loss function and gradient function:  \n",
    "- Loss function:\n",
    "$$\n",
    "J(\\theta)= \\frac{1}{m} \\sum_{i=1}^{m} \\left[\n",
    "    -y^{(i)} log \\left(\n",
    "        h_\\theta(x^{(i)})\n",
    "    \\right)\n",
    "    - (1-y^{(i)}) log \\left(\n",
    "        1-h_\\theta(x^{(i)})\n",
    "    \\right)\n",
    "\\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_{j}^2\n",
    "$$\n",
    "- Gradient function:\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m}\n",
    "\\left(\n",
    "    h_\\theta(x^{(i)}) - y^{(i)}\n",
    "\\right)\n",
    "x_{j}^{(i)} \\quad \\text{for } j=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left[\n",
    "    \\frac{1}{m} \\sum_{i=1}^{m}\n",
    "    \\left(\n",
    "        h_\\theta(x^{(i)}) - y^{(i)}\n",
    "    \\right)\n",
    "    x_{j}^{(i)}\n",
    "\\right] + \\frac{\\lambda}{m} \\theta_j \\quad \\text{for } j \\geq 1\n",
    "$$\n",
    "\n",
    "$$ \\text{where } \\lambda \\text{ is the hyperparameter to be tuned by a machine learning engineer.} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of L2 regularization is to turn down the weights of features which can heavily affect the model performance while it's doing Gradient Descent. These features is the main reason that cause overfitting. What the added term in the above formulas means is after an iteration of Gradient Descent, the stronger a feature is, the more its weight gets reduced. However, the regularized weights don't disappear completely, they just approach very close to 0. And so with just a slight modification to the original loss function, L2 regularization helps balancing the effectiveness of the data features, which in turn help to reduce overfitting.  \n",
    "\n",
    "An important thing to notice is that the regularization isn't applied on the weight value $\\theta_0$ (as mentioned in the above Gradient function formula), because it's actually the bias value of the model, not the weight of a feature in the training data.  \n",
    "\n",
    "Now let's create a logistic regression model applying the L2 regularization technique to perform classifcation on the toy dataset. We just need to make the loss function and gradient function, then the minimize() function from the scipy.optimize package can handle the gradient descent. For the Sigmoid function, we can use the scipy.special.expit() function since it's better optimized for some special cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(theta, X, y, lambda_):\n",
    "    \"\"\"Compute loss and gradient for logistic regression with L2 regularization.\n",
    "\n",
    "    Args:\n",
    "        theta (array_like): Logistic regression weights to be trained.\n",
    "            A vector with shape (n,1), where n is the number of features including any intercept. If we have mapped our initial features into polynomial features, then n is the total number of polynomial features.\n",
    "        \n",
    "        X (array_like): The dataset features with shape (m,n), where m is the number of samples and n is the number of features after feature mapping.\n",
    "        \n",
    "        y (array_like): The data labels, a vector with shape (m,1).\n",
    "        \n",
    "        lambda_ (float): The regularization hyperparameter.\n",
    "        \n",
    "    Returns:\n",
    "        J (float): The computed value for the regularized loss function.\n",
    "        \n",
    "        grad (array_like): A vector of shape (n,1) which is the gradients of the loss function with respect to theta, at the current values of theta.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0] # number of training samples\n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    epsilon = 1e-5 # To fix when the input of log function is too small that it becomes 0\n",
    "    \n",
    "    # Calculate sigmoid hypothesis\n",
    "    h = expit(X @ theta)\n",
    "    # Temporarily replace theta_0 with 0 to avoid regularization on the intercept/bias value\n",
    "    theta_0 = theta[0]\n",
    "    theta[0] = 0\n",
    "    \n",
    "    # Calculate loss value J and gradient\n",
    "    regTerm = (lambda_ / (2.0 * m)) * np.sum(theta ** 2)\n",
    "    J = (-1.0 * np.transpose(y) @ np.log(h + epsilon) - np.transpose(1.0 - y) @ np.log(1.0 - h + epsilon)) / m\n",
    "    J += regTerm\n",
    "    grad = (X.T @ (h - y)) / m + (lambda_ * theta) / m\n",
    "    \n",
    "    # Restore the original theta_0\n",
    "    theta[0] = theta_0\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function to calculate the loss and gradient at each descent step. Let's optimize a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dataclass to save the trained model\n",
    "@dataclass\n",
    "class Model:\n",
    "    cost: float\n",
    "    theta: np.ndarray\n",
    "    \n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        p = expit(X @ self.theta.T) >= 0.5\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, lambda_, iterations, minimize_method=\"TNC\"):\n",
    "    \"\"\"Fit a logistic regression model using the training data with L2 regularization.\n",
    "\n",
    "    Args:\n",
    "        X (array_like): The feature matrix of the training data.\n",
    "        \n",
    "        y (array_like): The label vector of the training data.\n",
    "        \n",
    "        lambda_ (float): The hyperparameter for tuning the L2 regularization, minimum is 0.\n",
    "        \n",
    "        iterations (int): Maximum number of iterations for the fitting process.\n",
    "        \n",
    "        minimize_method (str, optional): The method to fit the data. Defaults to \"TNC\". (See the documentations of scipy.optimize.minimize for available methods)\n",
    "        \n",
    "    Returns (Model):\n",
    "        A trained model with these attributes:\n",
    "        - cost (float): value of the loss function at the optimized theta\n",
    "        - theta (array_like): the optimized theta\n",
    "    \"\"\"\n",
    "    # Initialize fitting parameters\n",
    "    initial_theta = np.zeros(X.shape[1])\n",
    "    if lambda_ < 0:\n",
    "        lambda_ = 0\n",
    "        \n",
    "    # Set options for the minimize function\n",
    "    options = {\n",
    "        \"maxiter\": iterations\n",
    "    }\n",
    "    \n",
    "    res = optimize.minimize(l2_loss,\n",
    "                            initial_theta,\n",
    "                            (X, y, lambda_),\n",
    "                            jac=True,\n",
    "                            method=minimize_method\n",
    "                            options=options)\n",
    "    \n",
    "    # The \"fun\" property of OptimizeResult object returns:\n",
    "    #   fun: the value of loss function at optimized theta\n",
    "    #   x: the optimized theta\n",
    "    return Model(res.fun, res.x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f949023e22fc223d05ff4ad0c4b77cb805b04951b38b35fe9c48ac03e4aa352"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
